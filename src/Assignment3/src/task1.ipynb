{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Task 1**\n",
    " - Implement a LSTM (LSTM() and/or LSTMCell()) from scratch\n",
    " - Implement a Convolutional LSTM (ConvLSTM() and/or ConvLSTMCell()) from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size,kernel_size=3,padding=1, bias=True):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM cell.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_size: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "        \"\"\"\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Linear layer to compute all gates at once\n",
    "        # Note: 4 * hidden_size for i, f, o, g (input, forget, output, candidate)\n",
    "        \n",
    "        self.f_t = nn.Conv1d(in_channels=input_size + hidden_size,\n",
    "        out_channels=hidden_size,  # Matches hidden state dim\n",
    "        kernel_size=kernel_size,\n",
    "        padding=padding,\n",
    "        bias=bias # To maintain spatial dims\n",
    "        )\n",
    "        self.i_t =nn.Conv1d(in_channels=input_size + hidden_size,\n",
    "        out_channels=hidden_size,  # Matches hidden state dim\n",
    "        kernel_size=kernel_size,\n",
    "        padding=padding,\n",
    "        bias=bias # To maintain spatial dims\n",
    "        )\n",
    "\n",
    "        self.c_hat_t =nn.Conv1d(in_channels=input_size + hidden_size,\n",
    "        out_channels=hidden_size,  # Matches hidden state dim\n",
    "        kernel_size=kernel_size,\n",
    "        padding=padding,\n",
    "        bias=bias # To maintain spatial dims\n",
    "        )\n",
    "        \n",
    "        self.o_t = nn.Conv1d(in_channels=input_size + hidden_size,\n",
    "        out_channels=hidden_size,  # Matches hidden state dim\n",
    "        kernel_size=kernel_size,\n",
    "        padding=padding,\n",
    "        bias=bias # To maintain spatial dims\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(input_size + hidden_size, 4 * hidden_size)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
    "\n",
    "        \n",
    "        gates = self.linear(combined)  # Shape: (batch, 4 * hidden_size)\n",
    "        \n",
    "        # Split into input, forget, output, and candidate gates\n",
    "        #cc_i, cc_f, cc_o, cc_g = gates.chunk(4, dim=1)  # Each shape: (batch, hidden_size)\n",
    "        \n",
    "        f_t=torch.sigmoid(self.f_t(combined).T)\n",
    "        i_t=torch.sigmoid(self.i_t(combined).T)\n",
    "        c_hat_t=torch.tanh(self.c_hat_t(combined).T)\n",
    "        o_t=torch.sigmoid(self.o_t(combined).T)\n",
    "        \n",
    "        c_t=f_t*c_cur+i_t*c_hat_t\n",
    "        h_t=o_t*torch.tanh(c_t)\n",
    "        \n",
    "        return h_t, c_t\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_size, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_size, height, width, device=self.conv.weight.device))\n",
    "\n",
    "\n",
    "class LSTMWithCustomCell(nn.Module):\n",
    "    \"\"\" \n",
    "    Sequential classifier. Embedded images are fed to a RNN\n",
    "    Same as above, but using LSTMCells instead of the LSTM object\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "    emb_dim: integer \n",
    "        dimensionality of the vectors fed to the LSTM\n",
    "    hidden_dim: integer\n",
    "        dimensionality of the states in the cell\n",
    "    num_layers: integer\n",
    "        number of stacked LSTMS\n",
    "    mode: string\n",
    "        intialization of the states\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, emb_dim, hidden_dim, num_layers=1, mode=\"zeros\"):\n",
    "        \"\"\" Module initializer \"\"\"\n",
    "        assert mode in [\"zeros\", \"random\"]\n",
    "        super().__init__()\n",
    "        self.hidden_dim =  hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.mode = mode\n",
    "\n",
    "        # for embedding rows into vector representations\n",
    "        self.encoder = nn.Sequential(\n",
    "                nn.Conv2d(1, 64, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "                nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "                nn.Conv2d(128, emb_dim, 3, 1, 1),\n",
    "                nn.AdaptiveAvgPool2d((1, 1))\n",
    "            )\n",
    "        \n",
    "        # LSTM model       \n",
    "        lstms = []\n",
    "        for i in range(num_layers):\n",
    "            in_size = emb_dim if i == 0 else hidden_dim\n",
    "            #lstms.append( nn.LSTMCell(input_size=in_size, hidden_size=hidden_dim) )\n",
    "            lstms.append( ConvLSTMCell(input_size=in_size, hidden_size=hidden_dim) )\n",
    "            \n",
    "        self.lstm = nn.ModuleList(lstms)\n",
    "        \n",
    "        # FC-classifier\n",
    "        self.classifier = nn.Linear(in_features=hidden_dim, out_features=4)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass through model \"\"\"\n",
    "        \n",
    "        b_size, num_frames, n_channels, n_rows, n_cols = x.shape\n",
    "        h, c = self.init_state(b_size=b_size, device=x.device) \n",
    "        \n",
    "        # embedding rows\n",
    "        x = x.view(b_size * num_frames, n_channels, n_rows, n_cols)\n",
    "        embeddings = self.encoder(x)\n",
    "        embeddings = embeddings.reshape(b_size, num_frames, -1)\n",
    "        \n",
    "        # iterating over sequence length\n",
    "        lstm_out = []\n",
    "        for i in range(embeddings.shape[1]):  # iterate over time steps\n",
    "            lstm_input = embeddings[:, i, :]  # size= (batch_size, emb_dim) \n",
    "            # iterating over LSTM Cells\n",
    "            for j, lstm_cell in enumerate(self.lstm):\n",
    "                #try:\n",
    "                    if lstm_input.shape[0] != B_SIZE:\n",
    "                        continue\n",
    "                    #print(lstm_input.shape)\n",
    "                    h[j], c[j] = lstm_cell(lstm_input, (h[j], c[j]))\n",
    "                    lstm_input = h[j]\n",
    "                #except:\n",
    "                    #lstm_input=lstm_input;\n",
    "            lstm_out.append(lstm_input)\n",
    "        lstm_out = torch.stack(lstm_out, dim=1)\n",
    "            \n",
    "        # classifying\n",
    "        y = self.classifier(lstm_out[:, -1, :])  # feeding only output at last layer\n",
    "        \n",
    "        return y\n",
    "    \n",
    "        \n",
    "    def init_state(self, b_size, device):\n",
    "        \"\"\" Initializing hidden and cell state \"\"\"\n",
    "        if(self.mode == \"zeros\"):\n",
    "            h = [torch.zeros(b_size, self.hidden_dim).to(device) for _ in range(self.num_layers)]\n",
    "            c = [torch.zeros(b_size, self.hidden_dim).to(device) for _ in range(self.num_layers)]\n",
    "        elif(self.mode == \"random\"):\n",
    "            h = [torch.zeros(b_size, self.hidden_dim).to(device) for _ in range(self.num_layers)]\n",
    "            c = [torch.zeros(b_size, self.hidden_dim).to(device) for _ in range(self.num_layers)]\n",
    "        return h, c\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
