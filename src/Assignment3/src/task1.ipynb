{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Task 1**\n",
    " - Implement a LSTM (LSTM() and/or LSTMCell()) from scratch\n",
    " - Implement a Convolutional LSTM (ConvLSTM() and/or ConvLSTMCell()) from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from dataloader import KTHActionDataset\n",
    "from transformations import get_train_transforms,get_test_transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    \"model_name\" : \"OwnLSTM\",\n",
    "    \"exp\" : \"\",  # experiment label\n",
    "    \"emb_dim\" : 128,\n",
    "    \"hidden_dim\" : 128,\n",
    "    \"num_layers\" : 4,\n",
    "    \"batch_size\" : 32,\n",
    "    \"num_epochs\" : 60,\n",
    "    \"lr\" : 1e-3,\n",
    "    \"scheduler\" : \"StepLR\",\n",
    "    \"use_scheduler\" : False,\n",
    "    \"use_pretrained_encoder\" : False,\n",
    "    'max_frames' : 80,\n",
    "    'slicing_step' : 8,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir =\"../data/processed/\"\n",
    "\n",
    "train_dataset = KTHActionDataset(root_dir, \n",
    "                                 split=\"train\", \n",
    "                                 transform=get_train_transforms(configs['slicing_step']), \n",
    "                                 max_frames=configs['max_frames'], \n",
    "                                 img_size=(64, 64))\n",
    "\n",
    "test_dataset = KTHActionDataset(root_dir, \n",
    "                                split=\"test\", \n",
    "                                transform=get_test_transforms(configs['slicing_step']), \n",
    "                                # transform=None,\n",
    "                                max_frames=configs['max_frames'], \n",
    "                                img_size=(64, 64))\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, \n",
    "#                           batch_size=configs['batch_size'], \n",
    "#                           shuffle=True, \n",
    "#                           num_workers=4)\n",
    "\n",
    "# test_loader = DataLoader(test_dataset, \n",
    "#                          batch_size=configs['batch_size'], \n",
    "#                          shuffle=False, \n",
    "#                          num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/Learning/Cuda Vision Lab/CudaVisionSS2025/src/Assignment3/src/dataloader.py:46\u001b[0m, in \u001b[0;36mKTHActionDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# Get sequence directory and label\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     seq_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_dirs\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     47\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# Load frames\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size,kernel_size=3,padding=1, bias=True):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM cell.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_size: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "        \"\"\"\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Linear layer to compute all gates at once\n",
    "        # Note: 4 * hidden_size for i, f, o, g (input, forget, output, candidate)\n",
    "        \n",
    "        self.f_t = nn.Conv1d(in_channels=input_size + hidden_size,\n",
    "        out_channels=hidden_size,  # Matches hidden state dim\n",
    "        kernel_size=kernel_size,\n",
    "        padding=padding,\n",
    "        bias=bias # To maintain spatial dims\n",
    "        )\n",
    "        self.i_t =nn.Conv1d(in_channels=input_size + hidden_size,\n",
    "        out_channels=hidden_size,  # Matches hidden state dim\n",
    "        kernel_size=kernel_size,\n",
    "        padding=padding,\n",
    "        bias=bias # To maintain spatial dims\n",
    "        )\n",
    "\n",
    "        self.c_hat_t =nn.Conv1d(in_channels=input_size + hidden_size,\n",
    "        out_channels=hidden_size,  # Matches hidden state dim\n",
    "        kernel_size=kernel_size,\n",
    "        padding=padding,\n",
    "        bias=bias # To maintain spatial dims\n",
    "        )\n",
    "        \n",
    "        self.o_t = nn.Conv1d(in_channels=input_size + hidden_size,\n",
    "        out_channels=hidden_size,  # Matches hidden state dim\n",
    "        kernel_size=kernel_size,\n",
    "        padding=padding,\n",
    "        bias=bias # To maintain spatial dims\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(input_size + hidden_size, 4 * hidden_size)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
    "\n",
    "        \n",
    "        gates = self.linear(combined)  # Shape: (batch, 4 * hidden_size)\n",
    "        \n",
    "        # Split into input, forget, output, and candidate gates\n",
    "        #cc_i, cc_f, cc_o, cc_g = gates.chunk(4, dim=1)  # Each shape: (batch, hidden_size)\n",
    "        \n",
    "        f_t=torch.sigmoid(self.f_t(combined).T)\n",
    "        i_t=torch.sigmoid(self.i_t(combined).T)\n",
    "        c_hat_t=torch.tanh(self.c_hat_t(combined).T)\n",
    "        o_t=torch.sigmoid(self.o_t(combined).T)\n",
    "        \n",
    "        c_t=f_t*c_cur+i_t*c_hat_t\n",
    "        h_t=o_t*torch.tanh(c_t)\n",
    "        \n",
    "        return h_t, c_t\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_size, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_size, height, width, device=self.conv.weight.device))\n",
    "\n",
    "\n",
    "class LSTMWithCustomCell(nn.Module):\n",
    "    \"\"\" \n",
    "    Sequential classifier. Embedded images are fed to a RNN\n",
    "    Same as above, but using LSTMCells instead of the LSTM object\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "    emb_dim: integer \n",
    "        dimensionality of the vectors fed to the LSTM\n",
    "    hidden_dim: integer\n",
    "        dimensionality of the states in the cell\n",
    "    num_layers: integer\n",
    "        number of stacked LSTMS\n",
    "    mode: string\n",
    "        intialization of the states\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, emb_dim, hidden_dim, num_layers=1, mode=\"zeros\"):\n",
    "        \"\"\" Module initializer \"\"\"\n",
    "        assert mode in [\"zeros\", \"random\"]\n",
    "        super().__init__()\n",
    "        self.hidden_dim =  hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.mode = mode\n",
    "\n",
    "        # for embedding rows into vector representations\n",
    "        self.encoder = nn.Sequential(\n",
    "                nn.Conv2d(1, 64, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "                nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "                nn.Conv2d(128, emb_dim, 3, 1, 1),\n",
    "                nn.AdaptiveAvgPool2d((1, 1))\n",
    "            )\n",
    "        \n",
    "        # LSTM model       \n",
    "        lstms = []\n",
    "        for i in range(num_layers):\n",
    "            in_size = emb_dim if i == 0 else hidden_dim\n",
    "            #lstms.append( nn.LSTMCell(input_size=in_size, hidden_size=hidden_dim) )\n",
    "            lstms.append( ConvLSTMCell(input_size=in_size, hidden_size=hidden_dim) )\n",
    "            \n",
    "        self.lstm = nn.ModuleList(lstms)\n",
    "        \n",
    "        # FC-classifier\n",
    "        self.classifier = nn.Linear(in_features=hidden_dim, out_features=4)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass through model \"\"\"\n",
    "        \n",
    "        b_size, num_frames, n_channels, n_rows, n_cols = x.shape\n",
    "        h, c = self.init_state(b_size=b_size, device=x.device) \n",
    "        \n",
    "        # embedding rows\n",
    "        x = x.view(b_size * num_frames, n_channels, n_rows, n_cols)\n",
    "        embeddings = self.encoder(x)\n",
    "        embeddings = embeddings.reshape(b_size, num_frames, -1)\n",
    "        \n",
    "        # iterating over sequence length\n",
    "        lstm_out = []\n",
    "        for i in range(embeddings.shape[1]):  # iterate over time steps\n",
    "            lstm_input = embeddings[:, i, :]  # size= (batch_size, emb_dim) \n",
    "            # iterating over LSTM Cells\n",
    "            for j, lstm_cell in enumerate(self.lstm):\n",
    "                #try:\n",
    "                    if lstm_input.shape[0] != B_SIZE:\n",
    "                        continue\n",
    "                    #print(lstm_input.shape)\n",
    "                    h[j], c[j] = lstm_cell(lstm_input, (h[j], c[j]))\n",
    "                    lstm_input = h[j]\n",
    "                #except:\n",
    "                    #lstm_input=lstm_input;\n",
    "            lstm_out.append(lstm_input)\n",
    "        lstm_out = torch.stack(lstm_out, dim=1)\n",
    "            \n",
    "        # classifying\n",
    "        y = self.classifier(lstm_out[:, -1, :])  # feeding only output at last layer\n",
    "        \n",
    "        return y\n",
    "    \n",
    "        \n",
    "    def init_state(self, b_size, device):\n",
    "        \"\"\" Initializing hidden and cell state \"\"\"\n",
    "        if(self.mode == \"zeros\"):\n",
    "            h = [torch.zeros(b_size, self.hidden_dim).to(device) for _ in range(self.num_layers)]\n",
    "            c = [torch.zeros(b_size, self.hidden_dim).to(device) for _ in range(self.num_layers)]\n",
    "        elif(self.mode == \"random\"):\n",
    "            h = [torch.zeros(b_size, self.hidden_dim).to(device) for _ in range(self.num_layers)]\n",
    "            c = [torch.zeros(b_size, self.hidden_dim).to(device) for _ in range(self.num_layers)]\n",
    "        return h, c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
