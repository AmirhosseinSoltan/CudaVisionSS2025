{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Task 1**\n",
    " - Implement a LSTM (LSTM() and/or LSTMCell()) from scratch\n",
    " - Implement a Convolutional LSTM (ConvLSTM() and/or ConvLSTMCell()) from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Data #####\n",
    "data = \"\"\"To be, or not to be, that is the question: Whether \\\n",
    "'tis nobler in the mind to suffer The slings and arrows of ou\\\n",
    "trageous fortune, Or to take arms against a sea of troubles A\\\n",
    "nd by opposing end them. To die—to sleep, No more; and by a s\\\n",
    "leep to say we end The heart-ache and the thousand natural sh\\\n",
    "ocks That flesh is heir to: 'tis a consummation Devoutly to b\\\n",
    "e wish'd. To die, to sleep; To sleep, perchance to dream—ay, \\\n",
    "there's the rub: For in that sleep of death what dreams may c\\\n",
    "ome, When we have shuffled off this mortal coil, Must give us\\\n",
    " pause—there's the respect That makes calamity of so long lif\\\n",
    "e. For who would bear the whips and scorns of time, Th'oppres\\\n",
    "sor's wrong, the proud man's contumely, The pangs of dispriz'\\\n",
    "d love, the law's delay, The insolence of office, and the spu\\\n",
    "rns That patient merit of th'unworthy takes, When he himself \\\n",
    "might his quietus make\"\"\".lower()\n",
    "\n",
    "chars = set(data)\n",
    "\n",
    "data_size, char_size = len(data), len(chars)\n",
    "\n",
    "print(f'Data size: {data_size}, Char Size: {char_size}')\n",
    "\n",
    "char_to_idx = {c:i for i, c in enumerate(chars)}\n",
    "idx_to_char = {i:c for i, c in enumerate(chars)}\n",
    "\n",
    "train_X, train_y = data[:-1], data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Helper Functions #####\n",
    "def oneHotEncode(text):\n",
    "    output = np.zeros((char_size, 1))\n",
    "    output[char_to_idx[text]] = 1\n",
    "    return output\n",
    "\n",
    "def sigmoid(input, derivative = False):\n",
    "        if derivative:\n",
    "            return input * (1 - input)\n",
    "    \n",
    "        return 1 / (1 + np.exp(-input))\n",
    "\n",
    "def tanh(input, derivative = False):\n",
    "        if derivative:\n",
    "            return 1 - input ** 2\n",
    "        return np.tanh(input)\n",
    "def softmax(input):\n",
    "        return np.exp(input) / np.sum(np.exp(input))\n",
    "\n",
    "# Xavier Normalized Initialization\n",
    "def initWeights(input_size, output_size):\n",
    "    return np.random.uniform(-1, 1, (output_size, input_size)) * np.sqrt(6 / (input_size + output_size))\n",
    "\n",
    "\n",
    "\n",
    "class CustomLSTM:\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_epochs, learning_rate):\n",
    "        # Hyperparameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "        # Forget Gate\n",
    "        self.wf = initWeights(input_size, hidden_size)\n",
    "        self.bf = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Input Gate\n",
    "        self.wi = initWeights(input_size, hidden_size)\n",
    "        self.bi = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Candidate Gate\n",
    "        self.wc = initWeights(input_size, hidden_size)\n",
    "        self.bc = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Output Gate\n",
    "        self.wo = initWeights(input_size, hidden_size)\n",
    "        self.bo = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Final Gate\n",
    "        self.wy = initWeights(hidden_size, output_size)\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "    # Reset Network Memory\n",
    "    def reset(self):\n",
    "        self.concat_inputs = {}\n",
    "\n",
    "        self.hidden_states = {-1:np.zeros((self.hidden_size, 1))}\n",
    "        self.cell_states = {-1:np.zeros((self.hidden_size, 1))}\n",
    "\n",
    "        self.activation_outputs = {}\n",
    "        self.candidate_gates = {}\n",
    "        self.output_gates = {}\n",
    "        self.forget_gates = {}\n",
    "        self.input_gates = {}\n",
    "        self.outputs = {}\n",
    "    # Forward Propogation\n",
    "    def forward(self, inputs):\n",
    "        self.reset()\n",
    "\n",
    "        outputs = []\n",
    "        for q in range(len(inputs)):\n",
    "            self.concat_inputs[q] = np.concatenate((self.hidden_states[q - 1], inputs[q]))\n",
    "\n",
    "            self.forget_gates[q] = sigmoid(np.dot(self.wf, self.concat_inputs[q]) + self.bf)\n",
    "            self.input_gates[q] = sigmoid(np.dot(self.wi, self.concat_inputs[q]) + self.bi)\n",
    "            self.candidate_gates[q] = tanh(np.dot(self.wc, self.concat_inputs[q]) + self.bc)\n",
    "            self.output_gates[q] = sigmoid(np.dot(self.wo, self.concat_inputs[q]) + self.bo)\n",
    "\n",
    "            self.cell_states[q] = self.forget_gates[q] * self.cell_states[q - 1] + self.input_gates[q] * self.candidate_gates[q]\n",
    "            self.hidden_states[q] = self.output_gates[q] * tanh(self.cell_states[q])\n",
    "\n",
    "            outputs += [np.dot(self.wy, self.hidden_states[q]) + self.by]\n",
    "\n",
    "        return outputs\n",
    "# Backward Propogation\n",
    "    def backward(self, errors, inputs):\n",
    "        d_wf, d_bf = 0, 0\n",
    "        d_wi, d_bi = 0, 0\n",
    "        d_wc, d_bc = 0, 0\n",
    "        d_wo, d_bo = 0, 0\n",
    "        d_wy, d_by = 0, 0\n",
    "\n",
    "        dh_next, dc_next = np.zeros_like(self.hidden_states[0]), np.zeros_like(self.cell_states[0])\n",
    "        for q in reversed(range(len(inputs))):\n",
    "            error = errors[q]\n",
    "\n",
    "            # Final Gate Weights and Biases Errors\n",
    "            d_wy += np.dot(error, self.hidden_states[q].T)\n",
    "            d_by += error\n",
    "\n",
    "            # Hidden State Error\n",
    "            d_hs = np.dot(self.wy.T, error) + dh_next\n",
    "\n",
    "            # Output Gate Weights and Biases Errors\n",
    "            d_o = tanh(self.cell_states[q]) * d_hs * sigmoid(self.output_gates[q], derivative = True)\n",
    "            d_wo += np.dot(d_o, inputs[q].T)\n",
    "            d_bo += d_o\n",
    "\n",
    "            # Cell State Error\n",
    "            d_cs = tanh(tanh(self.cell_states[q]), derivative = True) * self.output_gates[q] * d_hs + dc_next\n",
    "\n",
    "            # Forget Gate Weights and Biases Errors\n",
    "            d_f = d_cs * self.cell_states[q - 1] * sigmoid(self.forget_gates[q], derivative = True)\n",
    "            d_wf += np.dot(d_f, inputs[q].T)\n",
    "            d_bf += d_f\n",
    "\n",
    "            # Input Gate Weights and Biases Errors\n",
    "            d_i = d_cs * self.candidate_gates[q] * sigmoid(self.input_gates[q], derivative = True)\n",
    "            d_wi += np.dot(d_i, inputs[q].T)\n",
    "            d_bi += d_i\n",
    "            \n",
    "            # Candidate Gate Weights and Biases Errors\n",
    "            d_c = d_cs * self.input_gates[q] * tanh(self.candidate_gates[q], derivative = True)\n",
    "            d_wc += np.dot(d_c, inputs[q].T)\n",
    "            d_bc += d_c\n",
    "\n",
    "            # Concatenated Input Error (Sum of Error at Each Gate!)\n",
    "            d_z = np.dot(self.wf.T, d_f) + np.dot(self.wi.T, d_i) + np.dot(self.wc.T, d_c) + np.dot(self.wo.T, d_o)\n",
    "\n",
    "            # Error of Hidden State and Cell State at Next Time Step\n",
    "            dh_next = d_z[:self.hidden_size, :]\n",
    "            dc_next = self.forget_gates[q] * d_cs\n",
    "\n",
    "        for d_ in (d_wf, d_bf, d_wi, d_bi, d_wc, d_bc, d_wo, d_bo, d_wy, d_by):\n",
    "            np.clip(d_, -1, 1, out = d_)\n",
    "\n",
    "        self.wf += d_wf * self.learning_rate\n",
    "        self.bf += d_bf * self.learning_rate\n",
    "\n",
    "        self.wi += d_wi * self.learning_rate\n",
    "        self.bi += d_bi * self.learning_rate\n",
    "\n",
    "        self.wc += d_wc * self.learning_rate\n",
    "        self.bc += d_bc * self.learning_rate\n",
    "\n",
    "        self.wo += d_wo * self.learning_rate\n",
    "        self.bo += d_bo * self.learning_rate\n",
    "\n",
    "        self.wy += d_wy * self.learning_rate\n",
    "        self.by += d_by * self.learning_rate\n",
    "    # Train\n",
    "    def train(self, inputs, labels):\n",
    "        inputs = [oneHotEncode(input) for input in inputs]\n",
    "\n",
    "        for _ in tqdm(range(self.num_epochs)):\n",
    "            predictions = self.forward(inputs)\n",
    "\n",
    "            errors = []\n",
    "            for q in range(len(predictions)):\n",
    "                errors += [-softmax(predictions[q])]\n",
    "                errors[-1][char_to_idx[labels[q]]] += 1\n",
    "\n",
    "            self.backward(errors, self.concat_inputs)\n",
    "    \n",
    "    # Test\n",
    "    def test(self, inputs, labels):\n",
    "        accuracy = 0\n",
    "        probabilities = self.forward([oneHotEncode(input) for input in inputs])\n",
    "\n",
    "        output = ''\n",
    "        for q in range(len(labels)):\n",
    "            prediction = idx_to_char[np.random.choice([*range(char_size)], p = softmax(probabilities[q].reshape(-1)))]\n",
    "\n",
    "            output += prediction\n",
    "\n",
    "            if prediction == labels[q]:\n",
    "                accuracy += 1\n",
    "\n",
    "        print(f'Ground Truth:\\nt{labels}\\n')\n",
    "        print(f'Predictions:\\nt{\"\".join(output)}\\n')\n",
    "        \n",
    "        print(f'Accuracy: {round(accuracy * 100 / len(inputs), 2)}%')\n",
    "# Initialize Network\n",
    "hidden_size = 25\n",
    "\n",
    "lstm = LSTM(input_size = char_size + hidden_size, hidden_size = hidden_size, output_size = char_size, num_epochs = 1_000, learning_rate = 0.05)\n",
    "\n",
    "##### Training #####\n",
    "lstm.train(train_X, train_y)\n",
    "\n",
    "##### Testing #####\n",
    "lstm.test(train_X, train_y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### LSTM Classifier #####\"\"\"\n",
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM Classifier, It is a wrap around our implemented custom LSTM module in PyTorch. \n",
    "    Embedded images are fed to a RNN\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "    emb_dim: integer \n",
    "        dimensionality of the vectors fed to the LSTM\n",
    "    hidden_dim: integer\n",
    "        dimensionality of the states in the cell\n",
    "    num_layers: integer\n",
    "        number of stacked LSTMS\n",
    "    mode: string\n",
    "        intialization of the states\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, emb_dim, hidden_dim, num_layers=1, mode=\"zeros\"):\n",
    "        \"\"\" Module initializer \"\"\"\n",
    "        assert mode in [\"zeros\", \"random\"]\n",
    "        super().__init__()\n",
    "        self.hidden_dim =  hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.mode = mode\n",
    "        \n",
    "        # for embedding rows into vector representations\n",
    "        self.encoder = nn.Sequential(\n",
    "                nn.Conv2d(1, 64, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "                nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "                nn.Conv2d(128, emb_dim, 3, 1, 1),\n",
    "                nn.AdaptiveAvgPool2d((1, 1))\n",
    "            )\n",
    "        \n",
    "        # LSTM model\n",
    "        self.lstm = CustomLSTM(\n",
    "                input_size=emb_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_epochs=1,\n",
    "                learning_rate=0.01\n",
    "            ) \n",
    "        \"\"\"=nn.LSTM(\n",
    "                input_size=emb_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=num_layers,\n",
    "                batch_first=True\n",
    "            )\"\"\"\n",
    "        \n",
    "        # FC-classifier\n",
    "        self.classifier = nn.Linear(in_features=hidden_dim, out_features=4)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass through model \"\"\"\n",
    "        \n",
    "        b_size, num_frames, n_channels, n_rows, n_cols = x.shape\n",
    "        h, c = self.init_state(b_size=b_size, device=x.device) \n",
    "        \n",
    "        # encoding all images in parallel rows\n",
    "        x = x.view(b_size * num_frames, n_channels, n_rows, n_cols)\n",
    "        embeddings = self.encoder(x)  # (b*T, C, H, W)  --> (B * T, emb_dim)\n",
    "        embeddings = embeddings.reshape(b_size, num_frames, -1)\n",
    "\n",
    "        # feeding LSTM. Does everything for you\n",
    "        lstm_out, (h_out, c_out) = self.lstm(embeddings, (h, c)) \n",
    "        \n",
    "        # classifying\n",
    "        y = self.classifier(lstm_out[:, -1, :])  # feeding only output at last layer\n",
    "        \n",
    "        return y\n",
    "    \n",
    "        \n",
    "    def init_state(self, b_size, device):\n",
    "        \"\"\" Initializing hidden and cell state \"\"\"\n",
    "        if(self.mode == \"zeros\"):\n",
    "            h = torch.zeros(self.num_layers, b_size, self.hidden_dim)\n",
    "            c = torch.zeros(self.num_layers, b_size, self.hidden_dim)\n",
    "        elif(self.mode == \"random\"):\n",
    "            h = torch.randn(self.num_layers, b_size, self.hidden_dim)\n",
    "            c = torch.randn(self.num_layers, b_size, self.hidden_dim)\n",
    "        h = h.to(device)\n",
    "        c = c.to(device)\n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
