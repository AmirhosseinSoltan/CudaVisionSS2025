{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Task 2**\n",
    " 1. Perform \"Action Recognition\" on the KTH-Actions dataset:\n",
    "    - https://www.csc.kth.se/cvap/actions/\n",
    "    - https://github.com/tejaskhot/KTH-Dataset\n",
    "    - Already downloaded in `/home/nfs/inf6/data/datasets/kth_actions`\n",
    "    - Use spatial dimensionality of frames of 64x64\n",
    "    - Split videos into subsequences of e.g. 10 frames. Treat each of these subsequences as independent.\n",
    "    - Feel free to use augmentations (temporal and/or spatial)\n",
    "    \n",
    "\n",
    "<br>\n",
    "\n",
    "2. Implement a model with the following structure:\n",
    "    - Convolutional encoder (probably more powerful than the one from above)\n",
    "    - Recurrent module\n",
    "    - Classifier (probably Conv + AvgPooling/Flattening + Linear)\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "3. Train, evaluate, and compare the model with the recurrent modules listed below.<br>\n",
    "   Compare the models in terms of accuracy, training/inference time, and number of learnable parameters.<br>\n",
    "   **Log the experiments on Tensorboard/WandB**. <br>\n",
    "   RNNs to evaluate:\n",
    "    - PyTorch LSTM model (using nn.LSTMCell)\n",
    "    - PyTorch GRU model (using nn.GRUCell)\n",
    "    - Your own LSTM\n",
    "    - Your own ConvLSTM\n",
    "    - **Note**: Different recurrent modules (e.g. LSTM vs ConvLSTM) might require slight changes in the encoder and classifier"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
