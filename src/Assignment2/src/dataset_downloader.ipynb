{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "import requests\n",
    "import shutil\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yt_dlp import YoutubeDL\n",
    "\n",
    "robot_videos = [ 'https://www.youtube.com/watch?v=gtJXGo8WEMY',\n",
    "                'https://www.youtube.com/watch?v=hKLC0Vz1GmM ',\n",
    "                'https://www.youtube.com/watch?v=6ldHWWHfeBc&t=5s',\n",
    "                'https://www.youtube.com/watch?v=WJKc56uUuF8',\n",
    "                'https://www.youtube.com/watch?v=RG205OwGdSg',\n",
    "                'https://www.youtube.com/watch?v=yVdB_0ry53o',\n",
    "                'https://www.youtube.com/watch?v=TWNvSHpMrSM',\n",
    "                'https://www.youtube.com/watch?v=UsmBD2_3FH8',\n",
    "                'https://www.youtube.com/watch?v=WGKo_6IkFBY',\n",
    "                'https://www.youtube.com/watch?v=G6xE7uWt6Fo&t=1s',\n",
    "                'https://www.youtube.com/watch?v=DrNcXgoFv20',\n",
    "                'https://www.youtube.com/watch?v=cpraXaw7dyc',\n",
    "                'https://www.youtube.com/watch?v=raYWbqbZbmc',\n",
    "                'https://www.youtube.com/watch?v=F_7IPm7f1vI]']\n",
    "\n",
    "root = os.getcwd()\n",
    "\n",
    "# Directories for youtube videos\n",
    "dataset_dir = os.path.join(root,'dataset')\n",
    "save_path = os.path.join(dataset_dir,'youtube_videos/')\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "\n",
    "# # Directories for dataset\n",
    "# os.makedirs(dataset_dir+'/robot/train', exist_ok=True)\n",
    "# os.makedirs(dataset_dir+'/robot/test', exist_ok=True)\n",
    "# os.makedirs(dataset_dir+'/person/train', exist_ok=True)\n",
    "# os.makedirs(dataset_dir+'/person/test', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to download frames from YouTube videos\n",
    "def youtube_downloader(urls,save_path):\n",
    "    ydl_opts = {  # Youtube downloader dictionaty options.\n",
    "        'outtmpl': save_path+'%(title)s.%(ext)s',  # Save path+video title+file extension\n",
    "        'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]',  # Prefer MP4\n",
    "    }\n",
    "    for url in urls:\n",
    "        with YoutubeDL(ydl_opts) as ydl:\n",
    "            try:\n",
    "                ydl.download([url])\n",
    "                print(\"Download complete!\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading video {url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_downloader(urls= robot_videos, save_path= save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, output_dir, num_frames , frames_collected):\n",
    "    '''\n",
    "    Extracts random number of frames from a given video file and saves \n",
    "    the resulting frames\n",
    "    '''\n",
    "\n",
    "    # Open video and get information\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Generating random frames to extract \n",
    "    random_frames = random.sample(range(0, total_frames), num_frames)\n",
    "    \n",
    "    # Extract frames\n",
    "    for i, frame_idx in enumerate(random_frames):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            # Save frame\n",
    "            frames_collected += 1\n",
    "            save_path = os.path.join(output_dir, f\"robot{frames_collected}.png\")\n",
    "            cv2.imwrite(save_path, frame)\n",
    "            \n",
    "            # # Break if collected enough frames\n",
    "            # if frames_collected >= num_frames:\n",
    "            #     break\n",
    "\n",
    "    # Release video\n",
    "    cap.release()\n",
    "    \n",
    "    return frames_collected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Robot images for train/test set extracted from youtube videos!\n"
     ]
    }
   ],
   "source": [
    "videos = glob.glob(save_path+'*.mp4', recursive= True)\n",
    "output_train = os.path.join(dataset_dir,'train','robot')\n",
    "output_test = os.path.join(dataset_dir,'test','robot')\n",
    "\n",
    "frames_collected_train = 0\n",
    "frames_collected_test = 0\n",
    "\n",
    "for video in videos:\n",
    "    # extracting num_frames number of random frames from each video \n",
    "    frames_collected_train = extract_frames(video_path= video, \n",
    "                                            output_dir= output_train, \n",
    "                                            num_frames= 25,\n",
    "                                            frames_collected= frames_collected_train)\n",
    "    \n",
    "    frames_collected_test = extract_frames(video_path= video, \n",
    "                                           output_dir= output_test, \n",
    "                                           num_frames= 10,\n",
    "                                           frames_collected= frames_collected_test)\n",
    "    \n",
    "print (\" Robot images for train/test set extracted from youtube videos!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "NUM_IMAGES_PER_CLASS = 180\n",
    "TRAIN_RATIO = 0.8\n",
    "PEXELS_API_KEY = \"Uf8MkiMbaC21S7H2AyZ6SS3urnALdpti85XPn7nxjnnNy7bfIe170fdl\"  # Replace with your Pexels API key\n",
    "TRAIN_DIR = os.path.join(dataset_dir, \"train\")\n",
    "TEST_DIR = os.path.join(dataset_dir, \"test\")\n",
    "\n",
    "# Create directory structure\n",
    "def create_dirs():\n",
    "    for dir_path in [os.path.join(TRAIN_DIR, \"human\"), os.path.join(TRAIN_DIR, \"robot\"),\n",
    "                     os.path.join(TEST_DIR, \"human\"), os.path.join(TEST_DIR, \"robot\")]:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Download images from Pexels for a given class\n",
    "def download_images(class_name, search_terms):\n",
    "    images = []\n",
    "    images_per_term = NUM_IMAGES_PER_CLASS // len(search_terms)\n",
    "    \n",
    "    headers = {\"Authorization\": PEXELS_API_KEY}\n",
    "    for term in search_terms:\n",
    "        page = 1\n",
    "        while len(images) < NUM_IMAGES_PER_CLASS:\n",
    "            url = f\"https://api.pexels.com/v1/search?query={term}&per_page={images_per_term}&page={page}\"\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Error fetching Pexels images for {term}: {response.status_code}\")\n",
    "                break\n",
    "            \n",
    "            data = response.json()\n",
    "            photos = data.get(\"photos\", [])\n",
    "            if not photos:\n",
    "                break\n",
    "                \n",
    "            for photo in photos:\n",
    "                img_url = photo[\"src\"][\"medium\"]\n",
    "                img_name = f\"{class_name}_{len(images)}_{term.replace(' ', '_')}.jpg\"\n",
    "                img_path = os.path.join(f\"temp_{class_name}\", img_name)\n",
    "                os.makedirs(f\"temp_{class_name}\", exist_ok=True)\n",
    "                \n",
    "                # Download image\n",
    "                img_response = requests.get(img_url, stream=True)\n",
    "                if img_response.status_code == 200:\n",
    "                    with open(img_path, \"wb\") as f:\n",
    "                        img_response.raw.decode_content = True\n",
    "                        shutil.copyfileobj(img_response.raw, f)\n",
    "                    images.append(img_path)\n",
    "                \n",
    "                if len(images) >= NUM_IMAGES_PER_CLASS:\n",
    "                    break\n",
    "            page += 1\n",
    "    \n",
    "    return images[:NUM_IMAGES_PER_CLASS]\n",
    "\n",
    "# Split and save images to train/test folders\n",
    "def split_and_save_images(images, class_name):\n",
    "    train_images, test_images = train_test_split(\n",
    "        images, train_size=TRAIN_RATIO, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Save train images\n",
    "    for idx, img_path in enumerate(train_images):\n",
    "        dest_path = os.path.join(TRAIN_DIR, class_name, f\"{class_name}_{idx}.jpg\")\n",
    "        shutil.copy(img_path, dest_path)\n",
    "    \n",
    "    # Save test images\n",
    "    for idx, img_path in enumerate(test_images):\n",
    "        dest_path = os.path.join(TEST_DIR, class_name, f\"{class_name}_{idx}.jpg\")\n",
    "        shutil.copy(img_path, dest_path)\n",
    "\n",
    "def main():\n",
    "    # Create directory structure\n",
    "    create_dirs()\n",
    "    \n",
    "    # Download human images\n",
    "    print(\"Downloading human images...\")\n",
    "    human_search_terms = [\"diverse people\", \"human portrait\", \"people in nature\", \"person\"]  # Diverse human images\n",
    "    human_images = download_images(\"human\", human_search_terms)\n",
    "    if len(human_images) < NUM_IMAGES_PER_CLASS:\n",
    "        print(f\"Warning: Only {len(human_images)} human images found.\")\n",
    "\n",
    "    # Split and save images\n",
    "    print(\"Organizing images into train/test folders...\")\n",
    "    split_and_save_images(human_images, \"human\")\n",
    "    # split_and_save_images(robot_images, \"robot\")\n",
    "    \n",
    "    # Clean up temporary folders\n",
    "    for temp_dir in [\"temp_human\", \"temp_robot\"]:\n",
    "        if os.path.exists(temp_dir):\n",
    "            shutil.rmtree(temp_dir)\n",
    "    \n",
    "    print(f\"Dataset created at {dataset_dir} with {NUM_IMAGES_PER_CLASS} images per class.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
